import traci
import sys
import os
from DQN_RL_Agent import DQNAgent # ç›´æ¥ import class
import csv # <--- æ–°å¢
from plyer import notification # <--- æ–°å¢

GA_RESULT_PATH = "./GA_best_result.csv"
last_total_waiting_time = 0.0
# --- æ–°å¢å…¨åŸŸè®Šæ•¸ ---
# context: åœ¨ RL_controller.py æª”æ¡ˆçš„é ‚éƒ¨æ–°å¢/ä¿®æ”¹æ­¤è¡Œ
last_total_queue_length = 0.0
last_total_cumulative_waiting_time = 0.0
def read_ga_optimal_phases(csv_filepath):
    """å¾ GA è¼¸å‡ºçš„ CSV æª”æ¡ˆä¸­è®€å–æœ€å¾Œä¸€è¡Œçš„ phase1 å’Œ phase2 æ•¸å€¼"""
    
    # é è¨­ç¶“é©—å€¼ (å¦‚æœæ‰¾ä¸åˆ°æª”æ¡ˆï¼Œå‰‡ç”¨ç¶“é©—å€¼ä»£æ›¿ï¼Œé¿å…ç¨‹å¼å´©æ½°)
    DEFAULT_PHASES = [35.0, 25.0] 

    if not os.path.exists(csv_filepath):
        print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ° GA çµæœæª”æ¡ˆ '{csv_filepath}'ï¼Œä½¿ç”¨é è¨­ç¶“é©—å€¼ {DEFAULT_PHASES} ä½œç‚ºéœæ…‹åŸºç·šã€‚")
        return DEFAULT_PHASES

    try:
        with open(csv_filepath, 'r') as f:
            reader = csv.reader(f)
            # ç¢ºä¿æª”æ¡ˆä¸ç‚ºç©º
            rows = list(reader)
            if len(rows) < 2: # è‡³å°‘è¦æœ‰æ¨™é ­å’Œä¸€è¡Œæ•¸æ“š
                print(f"è­¦å‘Šï¼š'{csv_filepath}' æª”æ¡ˆä¸­æ•¸æ“šä¸è¶³ï¼Œä½¿ç”¨é è¨­ç¶“é©—å€¼ {DEFAULT_PHASES}ã€‚")
                return DEFAULT_PHASES

            header = rows[0]
            last_row = rows[-1]
            
            # æ‰¾åˆ° phase1 å’Œ phase2 åœ¨ CSV ä¸­çš„æ¬„ä½ç´¢å¼•
            phase1_index = header.index('phase1')
            phase2_index = header.index('phase2')
            
            # è®€å–æ•¸å€¼ä¸¦è½‰ç‚º float
            phase1 = float(last_row[phase1_index])
            phase2 = float(last_row[phase2_index])
            print(f"âœ… æˆåŠŸå¾ '{csv_filepath}' è®€å– GA æœ€ä½³è§£ï¼š[{phase1}, {phase2}]")
            return [phase1, phase2]
            
    except Exception as e:
        # æ•ç²æ‰€æœ‰å¯èƒ½çš„éŒ¯èª¤ï¼Œä¾‹å¦‚æ ¼å¼éŒ¯èª¤ã€æ¬„ä½æ‰¾ä¸åˆ°ç­‰
        print(f"è®€å– GA çµæœæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ ({e})ï¼Œä½¿ç”¨é è¨­ç¶“é©—å€¼ {DEFAULT_PHASES}ã€‚")
        return DEFAULT_PHASES
    
# ã€DQN/GA æ··åˆç­–ç•¥çš„æ ¸å¿ƒæ•¸æ“šè¼¸å…¥ã€‘
# åœ¨ç¨‹å¼å•Ÿå‹•æ™‚å‘¼å«å‡½æ•¸ï¼Œå‹•æ…‹è¼‰å…¥æœ€ä½³æ™‚ç›¸
GA_OPTIMAL_PHASES = read_ga_optimal_phases(GA_RESULT_PATH)

def get_sumo_home():
    """æ‰¾åˆ° SUMO çš„å®‰è£…ç›®å½•å¹¶è¨­å®šç’°å¢ƒ"""
    if 'SUMO_HOME' in os.environ:
        tools = os.path.join(os.environ['SUMO_HOME'], 'tools')
        sys.path.append(tools)
        return True
    else:
        sys.exit("è«‹ç¢ºèª SUMO_HOME ç’°å¢ƒè®Šæ•¸å·²è¨­å®šï¼")

def get_state(tls_id):
    """
    ã€ä¿®å¾©ã€‘: ç²å–æŒ‡å®šäº¤é€šè™ŸèªŒçš„ç‹€æ…‹ï¼Œç´å…¥æ™‚ç›¸å’Œ GA æœ€ä½³è§£ã€‚
    """
    
    lanes = traci.trafficlight.getControlledLanes(tls_id)
    unique_lanes = list(set(lanes))
    
    # 1. ç²å–æ’éšŠé•·åº¦ (Queue Lengths)
    queue_lengths = [traci.lane.getLastStepHaltingNumber(lane) for lane in unique_lanes]
    
    # 2. ç²å–ç•¶å‰æ™‚ç›¸ (Current Phase) 
    current_phase = traci.trafficlight.getPhase(tls_id)
    
    # 3. ç•°æ§‹é›†æˆ (RL+GA) - ç¾åœ¨ä½¿ç”¨å‹•æ…‹è®€å–çš„æ•¸å€¼
    GA_min_time_suggestion = 0.0
    
    # å‡è¨­ Phase 0 æ˜¯ä¸»å¹¹é“ç¶ ç‡ˆï¼ŒPhase 2 æ˜¯æ¬¡å¹¹é“ç¶ ç‡ˆ
    if current_phase == 0:  
        GA_min_time_suggestion = GA_OPTIMAL_PHASES[0] # ä¸»å¹¹é“æœ€ä½³æ™‚é•·
    elif current_phase == 2: 
        GA_min_time_suggestion = GA_OPTIMAL_PHASES[1] # æ¬¡å¹¹é“æœ€ä½³æ™‚é•·
    
    # 4. çµ„åˆç‹€æ…‹ï¼šå°‡æ‰€æœ‰æ•¸å€¼æ‰å¹³åŒ–ç‚ºä¸€å€‹ tuple
    state_list = queue_lengths + [current_phase] + [GA_min_time_suggestion]
    
    return tuple(state_list)
# -----------------------------
    
# context: åœ¨ç¨‹å¼çš„å…¨å±€å€åŸŸ
# --- ä¿®æ­£å¾Œçš„ calculate_reward å‡½æ•¸ ---
# context: åœ¨ RL_controller.py æª”æ¡ˆçš„é ‚éƒ¨æ–°å¢/ä¿®æ”¹æ­¤è¡Œ
last_total_waiting_time = 0.0 # é‡æ–°å•Ÿç”¨é€™å€‹è®Šæ•¸ï¼Œä¸¦ç”¨ä¾†è¿½è¹¤ç´¯ç©ç­‰å¾…æ™‚é–“

def calculate_reward(tls_id):
    """
    è¨ˆç®—å³æ™‚çå‹µï¼šç¸½ç´¯ç©ç­‰å¾…æ™‚é–“çš„è®ŠåŒ–é‡ (Delta Delay)ã€‚
    ä½¿ç”¨ traci.vehicle.getWaitingTime ä¾†å¯¦ç¾æˆªåœ–ä¸­çš„ç›®æ¨™ã€‚
    """
    try:
        lanes = traci.trafficlight.getControlledLanes(tls_id)
        unique_lanes = list(set(lanes))
        
        current_total_waiting_time = 0.0
        
        # 1. éæ­·æ‰€æœ‰å—æ§è»Šé“
        for lane in unique_lanes:
            # ç²å–è©²è»Šé“ä¸Šæ‰€æœ‰è»Šè¼› ID
            vehicle_ids = traci.lane.getLastStepVehicleIDs(lane)
            
            # 2. éæ­·è»Šé“ä¸Šçš„æ‰€æœ‰è»Šè¼›
            for veh_id in vehicle_ids:
                # ã€é—œéµå˜—è©¦ã€‘ï¼šä½¿ç”¨ traci.vehicle ç´šåˆ¥çš„ API ä¾†ç²å–å–®è»Šç­‰å¾…æ™‚é–“
                # é€™å€‹ API æ‡‰è©²æ˜¯ç›¸ç•¶ç©©å®šçš„
                current_total_waiting_time += traci.vehicle.getWaitingTime(veh_id)
        
        # 3. è¨ˆç®— Delta Reward
        global last_total_waiting_time
        
        # Delta = (èˆŠçš„ç´¯ç©ç­‰å¾…æ™‚é–“ - æ–°çš„ç´¯ç©ç­‰å¾…æ™‚é–“)
        # å¦‚æœ Delta > 0ï¼Œè¡¨ç¤ºç­‰å¾…æ™‚é–“æ¸›å°‘ï¼Œçå‹µç‚ºæ­£ã€‚
        delta_delay = last_total_waiting_time - current_total_waiting_time
        delta_delay *= -1
        # ğŸŒŸ ã€é—œéµä¿®æ­£ã€‘ï¼šå¼•å…¥äºŒæ¬¡æ–¹æ’éšŠæ‡²ç½°
    # 1. ç²å–ç•¶å‰çš„ç¸½æ’éšŠè»Šè¼›æ•¸ (Halting Number)
        current_total_queue_length  = get_total_queue_length(tls_id)
        # 4. æ›´æ–°å…¨åŸŸè®Šæ•¸
        last_total_waiting_time = current_total_waiting_time
        # 2. æ‡²ç½°ä¿‚æ•¸ (å¯èª¿æ•´ 0.1 ~ 0.3)
        beta = 0.2 
        
        # 3. æœ€çµ‚çå‹µ = ç­‰å¾…æ™‚é–“è®ŠåŒ–çå‹µ - äºŒæ¬¡æ–¹æ’éšŠæ‡²ç½°
        reward =  delta_delay - (beta * (current_total_queue_length ** 2))
        # 5. å®šç¾©çå‹µï¼šæœ€å¤§åŒ–ç­‰å¾…æ™‚é–“çš„æ¸›å°‘
        
        # ç¬¬äºŒå€‹è¿”å›å€¼ 'current_total_waiting_time' å…¼å®¹ä¸»è¿´åœˆ
        return reward, current_total_waiting_time
            
    except traci.TraCIException:
        # SUMO é€£ç·šä¸­æ–·
        return 0.0, 0.0 
    except AttributeError as e:
        # æ•ç² 'LaneDomain' æˆ– 'VehicleDomain' ç›¸é—œçš„ AttributeError
        print(f"è¨ˆç®—çå‹µæ™‚ç™¼ç”Ÿè‡´å‘½ AttributeError: {e}. è«‹æª¢æŸ¥ traci.vehicle.getWaitingTime æ˜¯å¦å­˜åœ¨ã€‚", file=sys.stderr)
        # å¦‚æœé€™å€‹æ–¹æ³•å¤±æ•—ï¼Œå°±å›åˆ°æˆ‘å€‘ä¹‹å‰æœ€é­¯æ£’çš„ã€Œæ’éšŠé•·åº¦è®ŠåŒ–é‡ã€é‚è¼¯
        return calculate_reward_queue_fallback(tls_id)
    except Exception as e_general:
        # æ•ç²å…¶ä»–éŒ¯èª¤
        return 0.0, 0.0
def get_total_queue_length(tls_id):
    """
    è¨ˆç®—çµ¦å®šäº¤é€šè™ŸèªŒæ‰€æ§åˆ¶çš„æ‰€æœ‰è»Šé“ä¸Šçš„ç¸½æ’éšŠè»Šè¼›æ•¸ (Halting Number)ã€‚
    """
    try:
        # ç²å–äº¤é€šè™ŸèªŒæ§åˆ¶çš„æ‰€æœ‰è»Šé“
        lanes = traci.trafficlight.getControlledLanes(tls_id)
        # ä½¿ç”¨ set å»é™¤é‡è¤‡çš„è»Šé“ ID (å› ç‚ºä¸€å€‹ä¿¡è™Ÿç‡ˆå¯èƒ½æ§åˆ¶åŒä¸€è»Šé“çš„ä¸åŒéƒ¨åˆ†)
        unique_lanes = list(set(lanes))
        
        total_queue_length = 0
        
        # éæ­·æ‰€æœ‰å—æ§è»Šé“
        for lane in unique_lanes:
            # ç²å–ç•¶å‰æ­¥é©Ÿéœæ­¢æˆ–æ’éšŠçš„è»Šè¼›æ•¸ (é€Ÿåº¦å°æ–¼ 0.1 m/s)
            # traci.lane.getLastStepHaltingNumber(lane) æ˜¯è¨ˆç®—æ’éšŠé•·åº¦çš„æ¨™æº– API
            total_queue_length += traci.lane.getLastStepHaltingNumber(lane)
            
        return total_queue_length
            
    except traci.TraCIException:
        # SUMO é€£ç·šä¸­æ–·æˆ–å…¶ä»– traci éŒ¯èª¤
        return 0
    except Exception:
        # å…¶ä»–éŒ¯èª¤
        return 0

# (æ³¨æ„ï¼šé€™å€‹å‡½å¼éœ€è¦åœ¨æ‚¨çš„ç¨‹å¼ç¢¼ä¸­å®šç¾©ä¸€æ¬¡)
# --- å‚™ç”¨å‡½æ•¸ï¼šå¦‚æœ vehicle.getWaitingTime å¤±æ•—ï¼Œå‰‡å›é€€åˆ°æ’éšŠé•·åº¦ ---
# é€™æ˜¯ç¢ºä¿ç¨‹å¼ä¸æœƒå› ç‚º API ä¸ç›¸å®¹è€Œå´©æ½°çš„ä¿è­·å±¤
def calculate_reward_queue_fallback(tls_id):
    """
    å‚™ç”¨å‡½æ•¸ï¼šå¦‚æœåŸºæ–¼è»Šè¼›ç­‰å¾…æ™‚é–“çš„è¨ˆç®—å¤±æ•—ï¼Œå‰‡å›é€€åˆ°æ’éšŠé•·åº¦è®ŠåŒ–é‡ã€‚
    ï¼ˆä½¿ç”¨ä½ ä¸Šæ¬¡ä¿®æ­£å¾Œçš„ Delta Queue é‚è¼¯ï¼‰
    """
    print("ä½¿ç”¨calculate_reward_queue_fallback",flush=True)
    try:
        lanes = traci.trafficlight.getControlledLanes(tls_id)
        unique_lanes = list(set(lanes))
        
        current_total_queue_length = 0.0
        for lane in unique_lanes:
            current_total_queue_length += traci.lane.getLastStepHaltingNumber(lane)
        
        # æ³¨æ„ï¼šç‚ºäº†é¿å…ä¾è³´å¦ä¸€å€‹å…¨åŸŸè®Šæ•¸ï¼Œé€™è£¡æš«æ™‚ä½¿ç”¨ last_total_waiting_time ä½œç‚ºæ’éšŠé•·åº¦çš„è¿½è¹¤å™¨ã€‚
        # âš ï¸ é€™è£¡æ˜¯çŠ§ç‰²äº†è®Šæ•¸åç¨±çš„èªç¾©ï¼Œæ›å–ç¨‹å¼çš„é­¯æ£’æ€§ã€‚
        global last_total_waiting_time
        delta_queue = last_total_waiting_time - current_total_queue_length
        last_total_waiting_time = current_total_queue_length
        
        # ä½¿ç”¨ Delta Queue ä½œç‚ºçå‹µ
        reward = delta_queue * 1.0 
        
        # è¿”å› æ’éšŠé•·åº¦ (Queue Length)
        return reward, current_total_queue_length
            
    except traci.TraCIException:
        return 0.0, 0.0 
    except Exception as e_general:
        return 0.0, 0.0

def run_experiment():
    """ä¸»æ¨¡æ“¬è¿´åœˆ"""
    
    # --- 1. åˆå§‹åŒ–è¨­å®š ---
    SUMO_BINARY = "sumo" # é–‹ç™¼æ™‚ç”¨ sumo-gui, è¨“ç·´æ™‚ç”¨ sumo
    SUMOCFG_PATH = "./osm.sumocfg"
    SUMO_CMD = [SUMO_BINARY, "-c", SUMOCFG_PATH, "--quit-on-end", "--time-to-teleport", "-1"]
    
    TRAFFIC_LIGHT_ID = "1253678773"
    ACTION_SPACE = [0, 1]
    MIN_GREEN_TIME = 10
    DECISION_INTERVAL = 5
    
    # --- 2. å•Ÿå‹• SUMO ä¸¦ç²å–åˆå§‹è³‡è¨Š ---
    try:
        traci.start(SUMO_CMD)
        
        logics = traci.trafficlight.getAllProgramLogics(TRAFFIC_LIGHT_ID)
        num_phases = len(logics[0].phases) if logics else 4
        print(f"æˆåŠŸç²å–äº¤é€šè™ŸèªŒ '{TRAFFIC_LIGHT_ID}' çš„ç›¸ä½ç¸½æ•¸: {num_phases}")

        lanes = traci.trafficlight.getControlledLanes(TRAFFIC_LIGHT_ID)
        state_size = len(list(set(lanes))) + 2
        print(f"ç‹€æ…‹ç¶­åº¦ (State Size): {state_size}")
        
    except traci.TraCIException as e:
        print(f"å•Ÿå‹•SUMOæˆ–ç²å–è³‡è¨Šæ™‚å‡ºéŒ¯: {e}")
        traci.close()
        return

    # --- 3. åˆå§‹åŒ– Agent ---
    # --- ã€ä¿®æ­£é» Aï¼šå‚³é ID çµ¦ DQNAgentã€‘---
    # æ³¨æ„ï¼šRL_INSTANCE_ID å¿…é ˆåœ¨ RL_controller.py çš„æª”æ¡ˆé–‹é ­å¾ sys.argv è®€å–
    RL_INSTANCE_ID = "default_rl_instance" # æä¾›ä¸€å€‹é è¨­å€¼
    if len(sys.argv) > 1:
        # ä½¿ç”¨ sys.argv[1] ä½œç‚ºæœ€å¸¸è¦‹çš„å–®åƒæ•¸å‚³éæ–¹å¼
        RL_INSTANCE_ID = sys.argv[1] 
    print(f"ä½¿ç”¨çš„ RL å¯¦ä¾‹ ID (instance_id): {RL_INSTANCE_ID}")
    agent = DQNAgent(state_size=state_size, action_space=ACTION_SPACE, instance_id=RL_INSTANCE_ID)
    # å˜—è©¦è¼‰å…¥æ¨¡å‹ï¼Œå¦‚æœå­˜åœ¨çš„è©±
    agent.load_model() # <--- æ–°å¢ï¼šå˜—è©¦è¼‰å…¥æ¨¡å‹
    # --- 4. ä¸»æ¨¡æ“¬èˆ‡å­¸ç¿’è¿´åœˆ ---
    # context: åœ¨ run_experiment å‡½å¼çš„å…§éƒ¨
    
    # --- 4. ä¸»æ¨¡æ“¬èˆ‡å­¸ç¿’è¿´åœˆ ---
    step = 0
    # last_arrived_vehicles = 0 # è¿½è¸ªä¸Šä¸€è½®çš„åˆ°è¾¾è½¦è¾†æ•°

    while step < 5000:
        try:
            if traci.simulation.getMinExpectedNumber() <= 0:
                print("æ‰€æœ‰è»Šè¼›å·²é›¢é–‹æ¨¡æ“¬ï¼Œæå‰çµæŸã€‚")
                break

            current_state = get_state(TRAFFIC_LIGHT_ID)
            current_phase = traci.trafficlight.getPhase(TRAFFIC_LIGHT_ID)
            # ç²å–ç•¶å‰ç›¸ä½çš„ç¸½æ™‚é–“é•·åº¦ï¼ˆç¶ ç‡ˆæ™‚é–“ + é»ƒç‡ˆæ™‚é–“ï¼Œé€šå¸¸æ˜¯ 3sï¼‰
            phase_duration = traci.trafficlight.getPhaseDuration(TRAFFIC_LIGHT_ID)
           # ç²å–è·é›¢ä¸‹æ¬¡è®Šæ›å‰©é¤˜çš„æ™‚é–“ (Time to Next Switch)
            # é€™æ¨£æ¯”å˜—è©¦è¨ˆç®— elapsed time æ›´æº–ç¢ºä¸”å¸¸è¦‹
            time_remaining = traci.trafficlight.getNextSwitch(TRAFFIC_LIGHT_ID) - traci.simulation.getTime()
            # ç²å–ç•¶å‰ç›¸ä½æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼šrryyGGggrr...ï¼‰
            phase_state = traci.trafficlight.getRedYellowGreenState(TRAFFIC_LIGHT_ID)
            
            action = 0
            # åªæœ‰åœ¨ç¶ ç‡ˆç›¸ä½ (å¶æ•¸ç›¸ä½) ä¸”è¶…éæœ€çŸ­ç¶ ç‡ˆæ™‚é–“å¾Œæ‰å…è¨± RL æ±ºç­–
            if current_phase % 2 == 0 and time_remaining <= (phase_duration - MIN_GREEN_TIME - 3): # ç¨å¾®è°ƒæ•´åˆ¤æ–­é€»è¾‘
                action = agent.choose_action(current_state)

            if action == 1 and current_phase % 2 == 0:
                next_phase = (current_phase + 1) % num_phases
                traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, next_phase)
                
                # ç­‰å¾…é»ƒç‡ˆæ™‚é–“
                for _ in range(3):
                    traci.simulationStep()
                    
                    step += 1
            else:
                # (ä¿®æ­£ç‚¹): å¦‚æœå†³å®šç»´æŒï¼Œå°±è®©æ¨¡æ‹Ÿç»§ç»­è·‘ DECISION_INTERVAL æ­¥
                for _ in range(DECISION_INTERVAL):
                # åœ¨è¿›å…¥ä¸‹ä¸€æ­¥ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥æ¨¡æ‹Ÿæ˜¯å¦å·²ç»æ²¡æœ‰è½¦è¾†
                    if traci.simulation.getMinExpectedNumber() <= 0:
                        break # å¦‚æœæ²¡æœ‰è½¦äº†ï¼Œå°±è·³å‡ºè¿™ä¸ªå°å¾ªç¯
                    traci.simulationStep()
                    step += 1
                
            # --- å­¸ç¿’æ­¥é©Ÿ ---
            next_state = get_state(TRAFFIC_LIGHT_ID)
           
            reward, _ = calculate_reward(TRAFFIC_LIGHT_ID)
            
            agent.learn(current_state, action, reward, next_state)

            if step > 0:
                    
                print(f"æ™‚é–“: {step}s | çå‹µ: {reward:.2f} | Epsilon: {agent.exploration_rate:.3f}", flush=True)
                print(f"  > TL ç‹€æ…‹: Phase Index={current_phase}, State='{phase_state}', Duration={phase_duration:.1f}s, Time Remaining={time_remaining:.1f}s", flush=True)
        except traci.TraCIException:
            print("SUMO é€£ç·šä¸­æ–·ï¼Œæå‰çµæŸè¿´åœˆã€‚")
            break
    # --- 5. çµæŸæ¨¡æ“¬ ---
    print("æ­£åœ¨é—œé–‰æ¨¡æ“¬...")
    # --- ã€ä¿®æ­£é» Bï¼šå„²å­˜æœ€çµ‚æ¨¡å‹ã€‘---
    # ã€ä¿®æ­£é» 4ï¼šå„²å­˜æœ€çµ‚æ¨¡å‹ã€‘
    agent.save_model() # <--- æ–°å¢ï¼šå„²å­˜æ¨¡å‹
    traci.close()

# --- 1. æ–°å¢å‘½ä»¤åˆ—/äº’å‹•å¼åƒæ•¸è§£æå‡½æ•¸ ---
def parse_arguments():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸ï¼Œå…è¨±ä½¿ç”¨è€…é¸æ“‡æ¨¡å¼å’Œæ¨¡å‹åç¨±ã€‚"""
    
    # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„å‘½ä»¤è¡Œåƒæ•¸ (ä¾‹å¦‚: python RL_controller.py test my_model)
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
        if mode not in ['train', 'test']:
            print("âŒ éŒ¯èª¤: ç¬¬ä¸€å€‹åƒæ•¸å¿…é ˆæ˜¯ 'train' æˆ– 'test'ã€‚")
            sys.exit(1)
        instance_id = sys.argv[2]
    elif len(sys.argv) == 2: # è™•ç†åªçµ¦äº†æ¨¡å¼æˆ–IDçš„æƒ…æ³
        # ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å‡è¨­å¦‚æœåªæœ‰ä¸€å€‹åƒæ•¸ï¼Œå®ƒå°±æ˜¯æ¨¡å‹IDï¼Œä¸¦ä½¿ç”¨é»˜èªçš„è¨“ç·´æ¨¡å¼
        print(f"è­¦å‘Šï¼šåªæä¾›ä¸€å€‹åƒæ•¸ '{sys.argv[1]}'ï¼Œå°‡å…¶ä½œç‚ºæ¨¡å‹IDä¸¦é€²å…¥é»˜èªçš„è¨“ç·´æ¨¡å¼ã€‚")
        instance_id = sys.argv[1]
        
    return mode, instance_id


# def main():
#     mode, instance_id = parse_arguments()
#     is_train_mode = (mode == 'train')

#     # åƒæ•¸è¨­å®š
#     TRAFFIC_LIGHT_ID = "1253678773"
#     SUMO_CONFIG_FILE = "osm.sumocfg"
#     MAX_SIMULATION_STEPS = 100000 # æ¨¡æ“¬ç¸½æ­¥æ•¸
#     DECISION_INTERVAL = 5 # æ¯éš” 5 æ­¥é€²è¡Œä¸€æ¬¡æ±ºç­–
#     MIN_GREEN_TIME = 10 # æœ€å°ç¶ ç‡ˆæ™‚é–“
#     ACTION_SPACE = [0, 1]  # 0: Maintain, 1: Change Phase
    
#     # --- 2. åˆå§‹åŒ– DQN ä»£ç†ï¼Œä½¿ç”¨è§£æå‡ºçš„ instance_id ---
#     print(f"ä½¿ç”¨çš„ RL å¯¦ä¾‹ ID (instance_id): {instance_id}")
#     agent = DQNAgent(state_size=6, action_space=ACTION_SPACE, instance_id=instance_id) # state_size æš«æ™‚ç‚º 0


#     if is_train_mode:
#         print("ğŸ’¡ æ¨¡å¼ï¼šDQN è¨“ç·´æ¨¡å¼ (Train Mode)ã€‚")
#         # è¼‰å…¥ GA åŸºç·šæ•¸æ“š (è¨“ç·´ç”¨)
#         read_ga_optimal_phases(GA_RESULT_PATH)
#         # è¨“ç·´æ¨¡å¼æœƒç¹¼çºŒ Epsilon è¡°æ¸› (å¯ä»¥é¸æ“‡è¼‰å…¥ä¸Šæ¬¡é€²åº¦)
#         if agent.load_model():
#             print("âœ… æ‰¾åˆ°ä¸Šæ¬¡è¨“ç·´æ¨¡å‹ï¼Œå°‡ç¹¼çºŒè¨“ç·´ã€‚")
#         else:
#             print("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æª”æ¡ˆï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚")
            
#     else: # æ¸¬è©¦æ¨¡å¼
#         print("ğŸ’¡ æ¨¡å¼ï¼šDQN æ¸¬è©¦æ¨¡å¼ (Test Mode)ã€‚")
#         # --- æ¸¬è©¦æ¨¡å¼æ ¸å¿ƒé‚è¼¯ ---
#         if not agent.load_model():
#             print(f"\nâŒ è­¦å‘Šï¼šæ¸¬è©¦æ¨¡å¼ä¸‹æœªèƒ½æ‰¾åˆ°å·²è¨“ç·´çš„æ¨¡å‹æª”æ¡ˆ (ID: {instance_id})ï¼Œè«‹å…ˆåŸ·è¡Œè¨“ç·´ã€‚")
#             sys.exit(1) # æ¸¬è©¦æ¨¡å¼ä¸‹æ‰¾ä¸åˆ°æ¨¡å‹å°±é€€å‡º
            
#         agent.exploration_rate = 0.0 # é–å®šæ¢ç´¢ç‡ç‚º 0ï¼ŒåªåŸ·è¡Œåˆ©ç”¨(Exploitation)
#         print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸã€‚æ¢ç´¢ç‡ Epsilon å·²é–å®šç‚º {agent.exploration_rate}ã€‚")
        
        
#     # ... [å•Ÿå‹• SUMO å’Œ TraCI é€£ç·š]
#     if not get_sumo_home():
#         sys.exit(1)
#     # æ±ºå®šä½¿ç”¨çš„ç¨®å­ç¢¼
#     # è¨“ç·´æ™‚ä½¿ç”¨å›ºå®šç¨®å­ (ä¾‹å¦‚ 42)ï¼Œæ¸¬è©¦æ™‚ä½¿ç”¨ä¸åŒç¨®å­ (ä¾‹å¦‚ 100)
#     sim_seed = 42 if is_train_mode else 100 # <--- é€™è£¡å¯ä»¥å‹•æ…‹ä¿®æ”¹
#     # ã€ä¿®æ­£ã€‘: æ ¹æ“šæ¨¡å¼è‡ªå‹•é¸æ“‡ sumo æˆ– sumo-gui
#     sumo_binary = "sumo-gui" if is_train_mode else "sumo-gui"
#     sumoCmd = [
#         sumo_binary,
#         "-c", SUMO_CONFIG_FILE,
#         "--time-to-teleport", "-1",
#         "--tripinfo-output", "tripinfo.xml" ,
#         "--seed", str(sim_seed) # ã€æ–°å¢ã€‘åŠ å…¥éš¨æ©Ÿç¨®å­ç¢¼
#     ]
#     traci.start(sumoCmd)
    
    
#     # --- 3. åˆå§‹åŒ–ä¸¦é–‹å§‹æ¨¡æ“¬ ---
#     step = 0
#     cumulative_reward = 0.0
#     logics = traci.trafficlight.getAllProgramLogics(TRAFFIC_LIGHT_ID)
#     num_phases = len(logics[0].phases) if logics else 4
#     # ã€é—œéµä¿®æ­£ 1ã€‘ï¼šæ–°å¢æ™‚é–“è¿½è¹¤è®Šæ•¸
#     time_since_last_change = 0
#     # ã€ä¿®æ­£ã€‘: å‹•æ…‹ç²å– state_size ä¸¦å»ºç«‹æ¨¡å‹
#     lanes = traci.trafficlight.getControlledLanes(TRAFFIC_LIGHT_ID)
#     real_state_size = len(list(set(lanes))) + 2
#     agent.state_size = real_state_size
#     agent.build_models() # åœ¨ç²å–çœŸå¯¦ç¶­åº¦å¾Œï¼Œæ‰å»ºç«‹æ¨¡å‹

#     print(f"æˆåŠŸç²å–äº¤é€šè™ŸèªŒ '{TRAFFIC_LIGHT_ID}' çš„ç›¸ä½ç¸½æ•¸: {num_phases}")
#     print(f"ç‹€æ…‹ç¶­åº¦ (State Size): {agent.state_size}")

#     # --- 4. ä¸»æ¨¡æ“¬èˆ‡è¨“ç·´/æ¸¬è©¦è¿´åœˆ (æœ€çµ‚ç©©å®šçµæ§‹) ---
#     while step < MAX_SIMULATION_STEPS:
#         try:
#             # 1. æ¨é€²å–®æ­¥æ¨¡æ“¬èˆ‡æ™‚é–“è¨ˆæ•¸ (æ¯ä¸€æ­¥éƒ½åŸ·è¡Œ)
#             traci.simulationStep()
#             step += 1
#             time_since_last_change += 1 

#             # 2. æ±ºç­–èˆ‡å­¸ç¿’é‚è¼¯ï¼šåªåœ¨å›ºå®šçš„ DECISION_INTERVAL ç™¼ç”Ÿ
#             if step % DECISION_INTERVAL == 0:
                
#                 current_state = get_state(TRAFFIC_LIGHT_ID)
#                 current_phase = traci.trafficlight.getPhase(TRAFFIC_LIGHT_ID)
                
#                 action = 0 # é è¨­ï¼šç¶­æŒ
                
#                 # 2.1 å¿…é ˆæ˜¯ç¶ ç‡ˆï¼Œä¸”è¶…éæœ€å°ç¶ ç‡ˆæ™‚é–“ï¼Œæ‰å…è¨± Agent æ±ºç­–åˆ‡æ›
#                 if current_phase % 2 == 0 and time_since_last_change >= MIN_GREEN_TIME:
#                     action = agent.choose_action(current_state)
                    
#                 # 2.2 åŸ·è¡Œåˆ‡æ›å‹•ä½œ
#                 if action == 1:
#                     next_phase = (current_phase + 1) % num_phases
#                     traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, next_phase)
#                     time_since_last_change = 0 # é‡ç½®è¨ˆæ™‚å™¨
                
#                 # 2.3 å­¸ç¿’èˆ‡ç´€éŒ„ (ç™¼ç”Ÿåœ¨æ¯å€‹æ±ºç­–é»)
#                 next_state = get_state(TRAFFIC_LIGHT_ID)
#                 reward, current_total_queue_length = calculate_reward(TRAFFIC_LIGHT_ID)
                
#                 if is_train_mode:
#                     agent.learn(current_state, action, reward, next_state) 
                
#                 cumulative_reward += reward
#                 # 4. åŸ·è¡Œ RL å‹•ä½œ (åˆ‡æ›ç›¸ä½)
#                 # 2.4 å­¸ç¿’èˆ‡ç´€éŒ„ (ç™¼ç”Ÿåœ¨æ¯å€‹æ±ºç­–é»)
#                 next_state = get_state(TRAFFIC_LIGHT_ID)
#                 reward, current_total_queue_length = calculate_reward(TRAFFIC_LIGHT_ID)
                
#                 if is_train_mode:
#                     agent.learn(current_state, action, reward, next_state)
                
#                 cumulative_reward += reward
                
#                 # 2.5 è¼¸å‡ºç´€éŒ„
#                 time_info = f" | Phase Time: {time_since_last_change:.1f}s" # é€™è£¡è¼¸å‡ºçš„æ˜¯æ–°ç›¸ä½çš„é‹è¡Œæ™‚é–“
                
#                 if is_train_mode:
#                     status_line = f"æ™‚é–“: {step}s{time_info} | çå‹µ: {reward:.2f} | Action: {action} | Epsilon: {agent.exploration_rate:.3f}"
#                 else:
#                     status_line = f"æ™‚é–“: {step}s{time_info} | ç¬é–“çå‹µ: {reward:.2f} | æ’éšŠç¸½æ•¸: {current_total_queue_length:.2f}"
                
#                 print(status_line, flush=True)

#             # 3. è™•ç†é RL æ§åˆ¶çš„ç›¸ä½è·³è½‰ (é»ƒç‡ˆ -> ç´…ç‡ˆ/ç¶ ç‡ˆ)
#             #    ç”±æ–¼æˆ‘å€‘åªåœ¨ç¶ ç‡ˆæ™‚æ‰è®“ Agent æ±ºç­–åˆ‡æ›ï¼Œåœ¨é»ƒç‡ˆ/ç´…ç‡ˆæ™‚ï¼ŒSUMO æœƒè‡ªå‹•è·³è½‰ã€‚
#             #    æˆ‘å€‘åªéœ€è¦ç¢ºä¿åœ¨ SUMO è‡ªå‹•è·³è½‰å¾Œï¼Œtime_since_last_change èƒ½æ­£ç¢ºåæ˜ æ–°ç›¸ä½çš„é‹è¡Œæ™‚é–“ã€‚
#             #    (ç•¶ time_since_last_change é”åˆ°é»ƒç‡ˆ/ç´…ç‡ˆçš„é»˜èªæ™‚é•·æ™‚ï¼Œtraci.getPhase() æœƒè¿”å›æ–°å€¼ï¼Œ
#             #     time_since_last_change å‰‡å¾ 1 é–‹å§‹ç´¯ç©ï¼Œé‚è¼¯æ­£ç¢ºã€‚)

#                 # --- ç´€éŒ„èˆ‡è¼¸å‡º ---
#                 if step > 0:
#                     # ä½¿ç”¨æ­£ç¢ºçš„ time_since_last_change é€²è¡Œè¼¸å‡º
#                     time_info = f" | ç¶ ç‡ˆæ™‚é–“: {time_since_last_change:.1f}s"
                    
#                     if is_train_mode:
#                         status_line = f"æ™‚é–“: {step}s{time_info} | çå‹µ: {reward:.2f} | Epsilon: {agent.exploration_rate:.3f}"
#                     else:
#                         # æ¸¬è©¦æ¨¡å¼ä¸‹çš„ 'ç¸½ç­‰å¾…' å¯¦éš›ä¸Šæ˜¯æ’éšŠç¸½æ•¸ (current_total_queue_length)
#                         status_line = f"æ™‚é–“: {step}s{time_info} | ç¬é–“çå‹µ: {reward:.2f} | æ’éšŠç¸½æ•¸: {current_total_queue_length:.2f}"
                    
#                     print(status_line, flush=True)

#         except traci.TraCIException:
#             print("SUMO é€£ç·šä¸­æ–·ï¼Œæå‰çµæŸè¿´åœˆã€‚")
#             break
            
#     # --- 5. çµæŸæ¨¡æ“¬ ---
#     print("æ­£åœ¨é—œé–‰æ¨¡æ“¬...")
#     traci.close()
    
#     if is_train_mode:
#         agent.save_model() # è¨“ç·´çµæŸæ™‚å„²å­˜æ¨¡å‹
#     else:
#         # æ¸¬è©¦æ¨¡å¼ä¸‹çš„æœ€çµ‚çµæœè¼¸å‡º
#         print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼ä½¿ç”¨çš„æ¨¡å‹ ID: {instance_id}")
#         print(f"æ¨¡æ“¬ç¸½æ­¥æ•¸: {step}")
#         print(f"æœ€çµ‚ç´¯ç©çå‹µ: {cumulative_reward:.2f}")
#     notification.notify(
#         title = "Python RL Trainning Finish",
#         message = f"RUN PID: {os.getpid()}, MODEL ID= {instance_id}" ,
            
#         # displaying time
#         timeout=100 # seconds
#     )   
def main():
    mode, instance_id = parse_arguments()
    is_train_mode = (mode == 'train')

    # åƒæ•¸è¨­å®š
    TRAFFIC_LIGHT_ID = "1253678773"
    SUMO_CONFIG_FILE = "osm.sumocfg"
    MAX_SIMULATION_STEPS = 25000 # æ¨¡æ“¬ç¸½æ­¥æ•¸
    DECISION_INTERVAL = 5 # æ¯éš” 5 æ­¥é€²è¡Œä¸€æ¬¡æ±ºç­–
    MIN_GREEN_TIME = 10 # æœ€å°ç¶ ç‡ˆæ™‚é–“
    ACTION_SPACE = [0, 1]  # 0: Maintain, 1: Change Phase
    
    # --- 2. åˆå§‹åŒ– DQN ä»£ç†ï¼Œä½¿ç”¨è§£æå‡ºçš„ instance_id ---
    print(f"ä½¿ç”¨çš„ RL å¯¦ä¾‹ ID (instance_id): {instance_id}")
    agent = DQNAgent(state_size=6, action_space=ACTION_SPACE, instance_id=instance_id) # state_size æš«æ™‚ç‚º 0


    if is_train_mode:
        print("ğŸ’¡ æ¨¡å¼ï¼šDQN è¨“ç·´æ¨¡å¼ (Train Mode)ã€‚")
        # è¼‰å…¥ GA åŸºç·šæ•¸æ“š (è¨“ç·´ç”¨)
        read_ga_optimal_phases(GA_RESULT_PATH)
        # è¨“ç·´æ¨¡å¼æœƒç¹¼çºŒ Epsilon è¡°æ¸› (å¯ä»¥é¸æ“‡è¼‰å…¥ä¸Šæ¬¡é€²åº¦)
        if agent.load_model():
            print("âœ… æ‰¾åˆ°ä¸Šæ¬¡è¨“ç·´æ¨¡å‹ï¼Œå°‡ç¹¼çºŒè¨“ç·´ã€‚")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æª”æ¡ˆï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚")
            
    else: # æ¸¬è©¦æ¨¡å¼
        print("ğŸ’¡ æ¨¡å¼ï¼šDQN æ¸¬è©¦æ¨¡å¼ (Test Mode)ã€‚")
        # --- æ¸¬è©¦æ¨¡å¼æ ¸å¿ƒé‚è¼¯ ---
        if not agent.load_model():
            print(f"\nâŒ è­¦å‘Šï¼šæ¸¬è©¦æ¨¡å¼ä¸‹æœªèƒ½æ‰¾åˆ°å·²è¨“ç·´çš„æ¨¡å‹æª”æ¡ˆ (ID: {instance_id})ï¼Œè«‹å…ˆåŸ·è¡Œè¨“ç·´ã€‚")
            sys.exit(1) # æ¸¬è©¦æ¨¡å¼ä¸‹æ‰¾ä¸åˆ°æ¨¡å‹å°±é€€å‡º
            
        agent.exploration_rate = 0.0 # é–å®šæ¢ç´¢ç‡ç‚º 0ï¼ŒåªåŸ·è¡Œåˆ©ç”¨(Exploitation)
        print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸã€‚æ¢ç´¢ç‡ Epsilon å·²é–å®šç‚º {agent.exploration_rate}ã€‚")
        
        
    # ... [å•Ÿå‹• SUMO å’Œ TraCI é€£ç·š]
    if not get_sumo_home():
        sys.exit(1)
    # æ±ºå®šä½¿ç”¨çš„ç¨®å­ç¢¼
    # è¨“ç·´æ™‚ä½¿ç”¨å›ºå®šç¨®å­ (ä¾‹å¦‚ 42)ï¼Œæ¸¬è©¦æ™‚ä½¿ç”¨ä¸åŒç¨®å­ (ä¾‹å¦‚ 100)
    sim_seed = 42 if is_train_mode else 100 # <--- é€™è£¡å¯ä»¥å‹•æ…‹ä¿®æ”¹
    # ã€ä¿®æ­£ã€‘: æ ¹æ“šæ¨¡å¼è‡ªå‹•é¸æ“‡ sumo æˆ– sumo-gui
    sumo_binary = "sumo" if is_train_mode else "sumo-gui"
    sumoCmd = [
        sumo_binary,
        "-c", SUMO_CONFIG_FILE,
        "--time-to-teleport", "300",
        "--tripinfo-output", "tripinfo_RL_{}.xml" ,
        "--seed", str(sim_seed), # ã€æ–°å¢ã€‘åŠ å…¥éš¨æ©Ÿç¨®å­ç¢¼
        
        # ã€æ–°å¢ï¼šå•Ÿç”¨å­è»Šé“æ¨¡å‹ã€‘
        "--lateral-resolution", "0.05" # è¨­ç½®æ©«å‘è§£æåº¦ (ä¾‹å¦‚ï¼šæ¯ 0.2m ä¸€å€‹å­è»Šé“)
    ]
    traci.start(sumoCmd)
    
    
    # --- 3. åˆå§‹åŒ–ä¸¦é–‹å§‹æ¨¡æ“¬ ---
    step = 0
    cumulative_reward = 0.0
    logics = traci.trafficlight.getAllProgramLogics(TRAFFIC_LIGHT_ID)
    num_phases = len(logics[0].phases) if logics else 4
    # ã€é—œéµä¿®æ­£ 1ã€‘ï¼šæ–°å¢æ™‚é–“è¿½è¹¤è®Šæ•¸
    time_since_last_change = 0
    # ã€ä¿®æ­£ã€‘: å‹•æ…‹ç²å– state_size ä¸¦å»ºç«‹æ¨¡å‹
    lanes = traci.trafficlight.getControlledLanes(TRAFFIC_LIGHT_ID)
    real_state_size = len(list(set(lanes))) + 2
    agent.state_size = real_state_size
    agent.build_models() # åœ¨ç²å–çœŸå¯¦ç¶­åº¦å¾Œï¼Œæ‰å»ºç«‹æ¨¡å‹

    print(f"æˆåŠŸç²å–äº¤é€šè™ŸèªŒ '{TRAFFIC_LIGHT_ID}' çš„ç›¸ä½ç¸½æ•¸: {num_phases}")
    print(f"ç‹€æ…‹ç¶­åº¦ (State Size): {agent.state_size}")

    # --- 4. ä¸»æ¨¡æ“¬èˆ‡è¨“ç·´/æ¸¬è©¦è¿´åœˆ (æœ€çµ‚ç©©å®šçµæ§‹) ---
    while step < MAX_SIMULATION_STEPS:
        try:
            # 1. æª¢æŸ¥é€€å‡ºæ¢ä»¶
            if traci.simulation.getMinExpectedNumber() <= 0:
                print("æ‰€æœ‰è»Šè¼›å·²é›¢é–‹æ¨¡æ“¬ï¼Œæå‰çµæŸã€‚")
                break
                
            # 1. æ¨é€²å–®æ­¥æ¨¡æ“¬èˆ‡æ™‚é–“è¨ˆæ•¸ (æ¯ä¸€æ­¥éƒ½åŸ·è¡Œ)
            traci.simulationStep()
            step += 1
            time_since_last_change += 1 

            # 2. æ±ºç­–èˆ‡å­¸ç¿’é‚è¼¯ï¼šåªåœ¨å›ºå®šçš„ DECISION_INTERVAL ç™¼ç”Ÿ
            if step % DECISION_INTERVAL == 0:
                
                # 2.1 ç²å–ç‹€æ…‹ (åœ¨æ±ºç­–é»ç²å–)
                current_state = get_state(TRAFFIC_LIGHT_ID)
                current_phase = traci.trafficlight.getPhase(TRAFFIC_LIGHT_ID)
                
                action = 0 # é è¨­ï¼šç¶­æŒ
                
                # 2.2 åˆ¤æ–·æ˜¯å¦å…è¨± RL æ±ºç­– (å¿…é ˆæ˜¯ç¶ ç‡ˆï¼Œä¸”è¶…éæœ€å°ç¶ ç‡ˆæ™‚é–“)
                if current_phase % 2 == 0: # ç¢ºä¿åœ¨ç¶ ç‡ˆç›¸ä½ (0, 2, ...)
                    
                    if time_since_last_change >= MIN_GREEN_TIME:
                        action = agent.choose_action(current_state)
                    
                    # 2.3 åŸ·è¡Œåˆ‡æ›å‹•ä½œ
                    if action == 1:
                        # åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ç›¸ä½ (é»ƒç‡ˆ)
                        next_phase = (current_phase + 1) % num_phases
                        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, next_phase)
                        time_since_last_change = 0 # é‡ç½®è¨ˆæ™‚å™¨
                
                # 2.4 å­¸ç¿’èˆ‡ç´€éŒ„ (ç™¼ç”Ÿåœ¨æ¯å€‹æ±ºç­–é»)
                next_state = get_state(TRAFFIC_LIGHT_ID)
                reward, current_total_queue_length = calculate_reward(TRAFFIC_LIGHT_ID)
                
                if is_train_mode:
                    agent.learn(current_state, action, reward, next_state) 
                
                cumulative_reward += reward
                
                # 2.5 è¼¸å‡ºç´€éŒ„ (åªåœ¨æ±ºç­–é»è¼¸å‡º)
                time_info = f" | Phase Time: {time_since_last_change:.1f}s"
                phase_state = traci.trafficlight.getRedYellowGreenState(TRAFFIC_LIGHT_ID)
        
                if is_train_mode:
                    status_line = f"æ™‚é–“: {step}s{time_info} | çå‹µ: {reward:.2f} | States: {phase_state} | Action: {action} | Epsilon: {agent.exploration_rate:.3f}"
                else:
                    status_line = f"æ™‚é–“: {step}s{time_info} | ç¬é–“çå‹µ: {reward:.2f}  | States: {phase_state} | æ’éšŠç¸½æ•¸: {current_total_queue_length:.2f}"
                
                print(status_line, flush=True)

            # 3. è™•ç†é RL æ§åˆ¶çš„ç›¸ä½è·³è½‰ (é»ƒç‡ˆ -> ç´…ç‡ˆ/ç¶ ç‡ˆ)
            #    (æ­¤è™•ä¸éœ€è¦ä»»ä½•é¡å¤–çš„ç¨‹å¼ç¢¼ï¼Œç”± SUMO å…§éƒ¨è™•ç†)

        except traci.TraCIException:
            print("SUMO é€£ç·šä¸­æ–·ï¼Œæå‰çµæŸè¿´åœˆã€‚")
            break
            
    # --- 5. çµæŸæ¨¡æ“¬ ---
    print("æ­£åœ¨é—œé–‰æ¨¡æ“¬...")
    traci.close()
    
    if is_train_mode:
        agent.save_model() # è¨“ç·´çµæŸæ™‚å„²å­˜æ¨¡å‹
    else:
        # æ¸¬è©¦æ¨¡å¼ä¸‹çš„æœ€çµ‚çµæœè¼¸å‡º
        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼ä½¿ç”¨çš„æ¨¡å‹ ID: {instance_id}")
        print(f"æ¨¡æ“¬ç¸½æ­¥æ•¸: {step}")
        print(f"æœ€çµ‚ç´¯ç©çå‹µ: {cumulative_reward:.2f}")
    notification.notify(
        title = "Python RL Trainning Finish",
        message = f"RUN PID: {os.getpid()}, MODEL ID= {instance_id}" ,
            
        # displaying time
        timeout=100 # seconds
    )


# --- ç¨‹å¼é€²å…¥é» ---
if __name__ == "__main__":
    get_sumo_home()
    # run_experiment()
    # --- 6. å‘½ä»¤è¡Œåƒæ•¸è™•ç†ï¼šæ–°å¢ä¸€å€‹åƒæ•¸ä¾†æ§åˆ¶æ¨¡å¼ ---
    main()
    print("ç¨‹å¼åŸ·è¡Œå®Œç•¢ï¼")